{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786a4c25",
   "metadata": {},
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521e6f8",
   "metadata": {},
   "source": [
    "Линейная регрессия является одним из самых простых обучающихся алгоритмов\n",
    "в нашем инструментарии. Если вы когда-либо проходили вводный курс статистики\n",
    "в колледже, вероятно, последней темой, которую вы рассматривали, была линейная\n",
    "регрессия. На самом деле, она настолько проста, что иногда и вовсе не считается за\n",
    "машинное самообучение! Что бы вы ни думали, факт остается фактом: линейная\n",
    "регрессия — и ее расширения — продолжает оставаться распространенным и полезным методом предсказания, когда вектор целей является количественным\n",
    "значением (например, цена дома, возраст). В этой главе мы рассмотрим различные\n",
    "методы линейной регрессии (и некоторые расширения) для создания хорошо работающих предсказательных моделей.\n",
    "\n",
    "\n",
    "Регрессия — способ выбрать из семейства функций ту, которая минимизирует функцию потерь. Последняя характеризует насколько сильно пробная функция отклоняется от значений в заданных точках. Если точки получены в эксперименте, они неизбежно содержат ошибку измерений, шум, поэтому разумнее требовать, чтобы функция передавала общую тенденцию, а не точно проходила через все точки. В каком-то смысле регрессия — это «интерполирующая аппроксимация»: мы хотим провести кривую как можно ближе к точкам и при этом сохранить ее максимально простой чтобы уловить общую тенденцию. За баланс между этими противоречивыми желаниями как-раз отвечает функция потерь (в английской литературе «loss function» или «cost function»)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ab653",
   "metadata": {},
   "source": [
    "**Подгонка прямой**\n",
    "\n",
    "Требуется натренировать модель, представляющую линейную связь между признаком и вектором целей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45a369b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "# Загрузить данные только с двумя признаками\n",
    "california = fetch_california_housing()\n",
    "features = california.data[:,0:2]\n",
    "target = california.target\n",
    "# Создать объект линейной регрессии\n",
    "regression = LinearRegression()\n",
    "\n",
    "# Выполнить подгонку линейной регрессии\n",
    "model = regression.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123736f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10189032759082606"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Взглянуть на точку пересечения\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1473a97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43169191, 0.01744134])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Взглянуть на коэффициенты признаков\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dacdc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4526.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Первое значение в векторе целей, умноженное на 1000\n",
    "target[0]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1aa9d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4207.126263821179"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Предсказать целевое значение первого наблюдения, умноженное на 1000\n",
    "model.predict(features)[0]*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a911341",
   "metadata": {},
   "source": [
    "Основным преимуществом линейной регрессии является ее интерпретируемость\n",
    "в значительной степени потому, что модельные коэффициенты представляют собой эффект единичного изменения на вектор целей. Например, первый признак нашего\n",
    "решения— это количество преступлений на одного жителя. Коэффициент этого\n",
    "признака нашей модели составил -0.35. Это значит, что если мы умножим этот\n",
    "коэффициент на 1000 (т. к. вектором целей является цена дома в тысячах долларов),\n",
    "то у нас будет изменение в цене дома для каждого дополнительного преступления\n",
    "на душу населения:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2add2b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431.6919075449536"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Первый коэффициент, умноженный на 1000\n",
    "model.coef_[0]*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a25149",
   "metadata": {},
   "source": [
    "Это говорит о том, что каждое преступление на душу населения снизит цену дома\n",
    "примерно на $350!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca49533",
   "metadata": {},
   "source": [
    "**. Обработка интерактивных эффектов**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45049c9",
   "metadata": {},
   "source": [
    "Задача\n",
    "\n",
    "Имеется признак, влияние которого на целевую переменную зависит от другого\n",
    "признака.\n",
    "\n",
    "Решение\n",
    "\n",
    "Создать член взаимодействия для захвата этой зависимости с помощью объекта\n",
    "класса PolynomialFeatures библиотеки scikit-leam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300cdb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Загрузить данные только с двумя признаками\n",
    "california = fetch_california_housing()\n",
    "features = california.data[:,0:2]\n",
    "target = california.target\n",
    "# Создать член, характеризующий взаимодействие между признаками\n",
    "interaction = PolynomialFeatures(\n",
    "degree=3, include_bias=False, interaction_only=True)\n",
    "features_interaction = interaction.fit_transform(features)\n",
    "# Создать объект линейной регрессии\n",
    "regression = LinearRegression()\n",
    "# Выполнить подгонку линейной регрессии\n",
    "model = regression.fit(features_interaction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d314047",
   "metadata": {},
   "source": [
    "Функция PolynomialFeatures сгенерировала новую матрицу показателей, состоящую из всех их полиномиальных комбинаций со степенью меньше или равной указанной (в нашем примере 2). Затем мы нормализовали эти данные и скормили их нашей модели.\n",
    "\n",
    "Иногда влияние признака на целевую переменную по крайней мере частично зависит от другого признака. Например, представьте себе простой пример с кофе, где\n",
    "у нас два бинарных признака — наличие сахара (sugar) и было ли выполнено перемешивание (stirred) — и требуется предсказать, сладкий ли кофе на вкус. Просто\n",
    "положив сахар в кофе (sugar=l, stirred=0), мы не сделаем вкус кофе сладким (весь\n",
    "сахар на дне!), а одно перемешивание кофе без добавления сахара (sugar=o,\n",
    "stirred=i) не сделает его сладким. Как раз наоборот, именно взаимодействие брошенного в кофе сахара и перемешивания кофе (sugar=i, stirred=i) сделает вкус кофе сладким. Влияния сахара и перемешивания на сладость зависят друг от друга.\n",
    "В этом случае мы говорим, что существует эффект взаимодействия между признаками sugar И stirred.\n",
    "\n",
    "\n",
    "Мы можем учесть эффекты взаимодействия, включив новый признак, состоящий из\n",
    "произведения соответствующих значений из взаимодействующих признаков:\n",
    "\n",
    "У* = Ро + Р1*х + р2х, + рзх1х2 + е\n",
    "\n",
    "где х1 и х2 — значения соответственно sugar и stirred; х1х2 — взаимодействие\n",
    "между ними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b456a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.3252, 41.    ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Взглянуть на признаки для первого наблюдения\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f764af",
   "metadata": {},
   "source": [
    "Для того чтобы создать член, характеризующий взаимодействие между признаками, мы просто умножаем эти два значения между собой для каждого наблюдения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53e4c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341.33320000000003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импортировать библиотеку\n",
    "import numpy as np\n",
    "# Для каждого наблюдения перемножить значения\n",
    "# первого и второго признаков\n",
    "interaction_term = np.multiply(features[:, 0], features[:, 1])\n",
    "#Затем мы можем взглянуть на член взаимодействия для первого наблюдения:\n",
    "# Взглянуть на член взаимодействия для первого наблюдения\n",
    "interaction_term[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613f477",
   "metadata": {},
   "source": [
    "Для создания членов взаимодействия с использованием объекта PolynomiaiFeatures\n",
    "необходимо задать три важных параметра. Самый главный interaction_oniy=True\n",
    "сообщает объекту PolynomiaiFeatures возвращать только члены взаимодействия\n",
    "(а не полиномиальные признаки, которые мы обсудим в рецепте 13.3). По умолчанию класс PolynomiaiFeatures добавляет признак, содержащий те, которые называются смещением. Мы можем предотвратить это с помощью inciude_bias=Faise. Наконец, параметр degree определяет максимальное количество признаков для создания членов взаимодействия (в случае, если мы хотим создать член взаимодействия,\n",
    "который является сочетанием трех признаков). Результат работы объекта класса\n",
    "PolynomiaiFeatures можно увидеть из нашего решения, проверив, соответствуют ли\n",
    "значения признаков первого наблюдения значению члена взаимодействия нашей\n",
    "вручную рассчитанной версии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1508c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.3252,  41.    , 341.3332])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Взглянуть на значения для первого наблюдения\n",
    "features_interaction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1beb43",
   "metadata": {},
   "source": [
    "** Подгонка нелинейной связи**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e72cc",
   "metadata": {},
   "source": [
    "Задача\n",
    "\n",
    "Требуется смоделировать нелинейную связь.\n",
    "\n",
    "Решение\n",
    "\n",
    "Создать полиномиальную регрессию путем включения полиномиальных признаков\n",
    "в линейную регрессионную модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1062dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Загрузить данные с одним признаком\n",
    "california = fetch_california_housing()\n",
    "features = california.data[:,0:1]\n",
    "target = california.target\n",
    "# Создать полиномиальные признаки хл2 и хл3\n",
    "polynomial = PolynomialFeatures(degree=3, include_bias=False)\n",
    "features_polynomial = polynomial.fit_transform(features)\n",
    "# Создать объект линейной регрессии\n",
    "regression = LinearRegression()\n",
    "# Выполнить подгонку линейной регрессии\n",
    "model = regression.fit(features_polynomial, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8d5a5",
   "metadata": {},
   "source": [
    "До сих пор мы обсуждали моделирование только линейных связей. Примером линейной связи может служить количество этажей здания и его высота. В линейной\n",
    "регрессии мы исходим из того, что эффект количества этажей и высоты здания\n",
    "примерно постоянен, а это означает, что 20-этажное здание будет примерно в два\n",
    "раза выше, чем 10-этажное, которое будет примерно в два раза выше, чем 5-этажное здание. Однако многие представляющие интерес связи не являются строго\n",
    "линейными.\n",
    "\n",
    "Нередко требуется смоделировать нелинейную связь — например, связь между количеством часов, которые учащийся тратит на учебу, и оценкой, которую он получает за контрольную работу. Интуитивно мы можем себе представить, что в оценках между учащимися, которые учились в течение одного часа, и учащимися, которые вообще не учились, есть большая разница. Вместе с тем существует гораздо\n",
    "меньшая разница в оценках между учащимся, который учился в течение 99 часов,\n",
    "и учащимся, который учился в течение 100 часов. Влияние одного часа учебы на\n",
    "итоговую оценку учащегося уменьшается по мере увеличения количества часов.\n",
    "Полиномиальная регрессия — это расширение линейной регрессии, позволяющее\n",
    "моделировать нелинейные связи. Для того чтобы создать полиномиальную регрессию, следует конвертировать линейную функцию, которую мы использовали в рецепте 13.1:\n",
    "\n",
    "где d — степень полинома. Как использовать линейную регрессию для нелинейного признака? Ответ заключается в том, что мы ничего не изменяем в том, как\n",
    "линейная регрессия подбирает модель, а только добавляем полиномиальные признаки. То есть, линейная регрессия не \"знает\", что х 2 является квадратичным преобразованием х. Она просто рассматривает его, как еще одну переменную.\n",
    "\n",
    "Более практичное описание может лежать в порядке. Для того чтобы смоделировать нелинейные связи, мы можем создать новые признаки, которые возводят\n",
    "существующий признак, х, в некоторую степень: х2, х3 и т. д. Чем больше этих\n",
    "новых признаков мы добавим, тем более гибкой будет \"линия\", созданная нашей\n",
    "моделью. Для того чтобы более четко отразить суть, представьте, что мы хотим\n",
    "создать полином третьей степени. Для простоты мы сосредоточимся только на\n",
    "одном наблюдении (первом наблюдении в наборе данных) х0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fef2d0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.3252])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Взглянуть на первое наблюдение\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ea166",
   "metadata": {},
   "source": [
    "Для того чтобы создать полиномиальный признак, мы возведем первое значение\n",
    "наблюдения во вторую степень — х^2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43c1c9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69.30895504])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Взглянуть на первое наблюдение, возведенное во вторую степень, хА2\n",
    "features[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5abb5",
   "metadata": {},
   "source": [
    "Включив все три признака (х, х2 и х3) в матрицу признаков, а затем выполнив\n",
    "линейную регрессию, мы провели полиномиальную регрессию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d82cc1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.3252    ,  69.30895504, 577.0109125 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Взглянуть на значения первого наблюдения для х, хА2 и хА3\n",
    "features_polynomial[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe55a62",
   "metadata": {},
   "source": [
    "Полиномиальные признаки имеют два важных параметра. Во-первых, degree определяет максимальное число степеней для полиномиальных признаков. Например, degree=3 будет генерировать х2 и х3. Наконец, по умолчанию объект\n",
    "Polynomial Features включает в себя признаки, содержащие одни единицы (называемые Смещением). Мы МОЖеМ удаЛИТЬ ИХ, установив include_bias=False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e31261",
   "metadata": {},
   "source": [
    "**Снижение дисперсии\n",
    "с помощью регуляризации**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2fdad",
   "metadata": {},
   "source": [
    "Требуется уменьшить дисперсию линейной регрессионной модели.\n",
    "\n",
    "Использовать обучающийся алгоритм, который включает сжимающий штраф (так\n",
    "называемую регуляризацию), такой как гребневая регрессия и лассо-регрессия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0db3ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Загрузить данные\n",
    "california = fetch_california_housing()\n",
    "features = california.data\n",
    "target = california.target\n",
    "# Стандартизировать признаки\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "# Создать объект гребневой регрессии со значением альфа\n",
    "regression = Ridge(alpha=0.5)\n",
    "# Выполнить подгонку линейной регрессии\n",
    "model = regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038341d3",
   "metadata": {},
   "source": [
    "В стандартной линейной регрессии модель тренируется минимизировать сумму\n",
    "квадратической ошибки (погрешности) между истинными у, и предсказываемыми у, целевыми значениями, или остаточную сумму квадратов (residual sum of\n",
    "squares, RSS):\n",
    "    \n",
    "    RSS = sum(yi-<u>yi</u>)^2\n",
    "Регуляризованные регрессионные ученики похожи, за исключением того, что они\n",
    "пытаются минимизировать RSS и некий штраф за общий размер значений коэффициентов, называемый сжимающим штрафом, потому что он пытается \"сжать\" модель. Существуют два распространенных типа регуляризованных учеников для линейной регрессии: гребневая регрессия и лассо. Единственным формальным различием между ними является тип применяемого сжимающего штрафа. В гребневой\n",
    "регрессии сжимающий штраф — это настроечный гиперпараметр, умноженный на\n",
    "квадрат суммы всех коэффициентов:\n",
    "\n",
    "    RSS + alpha* sum(beta** 2)\n",
    "    \n",
    "где betai — коэффициент j -го из р признаков; аlpha — гиперпараметр (обсуждается далее). Лассо-регрессия очень похожа на гребневую регрессию, за исключением того,\n",
    "что сжимающий штраф — это настроечный параметр, умножаемый на сумму абсолютных значений всех коэффициентов:\n",
    "\n",
    "    1/2n * RSS + alpha * sum(|<u>b</u>i|)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcebbd92",
   "metadata": {},
   "source": [
    "где п — количество наблюдений. Так какой же из них следует использовать? В качестве очень общего эмпирического правила нужно учитывать, что гребневая perрессия часто дает несколько лучшие предсказания, чем лассо, но лассо (по причинам, которые мы обсудим в рецепте 13.5) производит более интерпретируемые модели. Если мы хотим сбалансировать штрафные функции между гребнем и лассо,\n",
    "мы можем использовать эластичную сеть, представляющую собой регрессионную\n",
    "модель, в которую включены оба штрафа. Независимо от того, какой из них мы используем, гребневая регрессия и лассо-регрессия могут штрафовать большие или\n",
    "сложные модели, включая значения коэффициентов в функцию потери, которую\n",
    "мы пытаемся минимизировать.\n",
    "\n",
    "Гиперпараметр а позволяет нам контролировать то, насколько мы штрафуем коэффициенты, где более высокие значения а создают более простые модели. Идеальное значение а должно быть настроено, как и любой другой гиперпараметр.\n",
    "В библиотеке scikit-leam а устанавливается с помощью параметра alpha.\n",
    "\n",
    "Библиотека scikit-leam включает класс Ridgecv, реализующий метод, который позволяет отбирать идеальное значение для а :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b78df66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8293461 ,  0.11939823, -0.26422311,  0.30398067, -0.00427544,\n",
       "       -0.03936068, -0.8937389 , -0.86433656])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить библиотеку\n",
    "from sklearn.linear_model import RidgeCV\n",
    "# Создать объект гребневой регрессии с тремя значениями alpha\n",
    "regr_cv = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "# Выполнить подгонку линейной регрессии\n",
    "model_cv = regr_cv.fit(features_standardized, target)\n",
    "# Взглянуть на коэффициенты\n",
    "model_cv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11ac03b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Взглянуть на alpha\n",
    "model_cv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6baf7a1",
   "metadata": {},
   "source": [
    "И последнее замечание: поскольку в линейной регрессии значение коэффициентов\n",
    "частично определяется шкалой признака, а в регуляризованных моделях все коэффициенты суммируются вместе, перед тренировкой мы должны убедиться, что\n",
    "стандартизировали признак."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573569f",
   "metadata": {},
   "source": [
    " **Уменьшение количества признаков\n",
    "с помощью лассо-регрессии**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40caff6",
   "metadata": {},
   "source": [
    "Использовать лассо-регрессию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a83abd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить библиотеки\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Загрузить данные\n",
    "california = fetch_california_housing()\n",
    "features = california.data\n",
    "target = california.target\n",
    "# Стандартизировать признаки\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "# Создать объект лассо-регрессии со значением alpha\n",
    "regression = Lasso(alpha=0.5)\n",
    "# Выполнить подгонку линейной регрессии\n",
    "model = regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc9760",
   "metadata": {},
   "source": [
    "Одна из интересных характеристик штрафа лассо-регрессии состоит в том, что он\n",
    "может сжимать коэффициенты модели до нуля, эффективно уменьшая количество\n",
    "признаков в модели. Например, в нашем решении мы установили alpha на уровне\n",
    "0.5 и видим, что многие коэффициенты равны 0. Иными словами, их соответствующие признаки в модели не используются:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97d1302d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.29398939,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Взглянуть на коэффициенты\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5c1ecfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., -0., -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать лассо-регрессию с высоким alpha\n",
    "regression_a10 = Lasso(alpha=10)\n",
    "model_a10 = regression_a10.fit(features_standardized, target)\n",
    "model_a10.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b1226",
   "metadata": {},
   "source": [
    "Однако, если мы увеличим а до гораздо более высокого значения, мы увидим, что\n",
    "буквально ни один из признаков не используется\n",
    "\n",
    "Практическая выгода от этого эффекта в следующем: он означает, что мы можем\n",
    "включить в нашу матрицу признаков 100 признаков, а затем путем настройки\n",
    "гиперпараметра а лассо-регрессии создать модель, которая использует только (например) 10 наиболее важных признаков. Это позволяет нам уменьшить дисперсию,\n",
    "улучшая интерпретируемость нашей модели (поскольку меньшее количество признаков легче объяснить)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d3b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
